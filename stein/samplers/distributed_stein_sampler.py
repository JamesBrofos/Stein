import numpy as np
import tensorflow as tf
import threading as th
from .abstract_stein_sampler import AbstractSteinSampler
from .stein_sampler import SteinSampler


class DistributedSteinSampler(AbstractSteinSampler):
    """Distributed Stein Sampler Class

    The parallel Stein sampler class exploits the fact that we can obtain a
    stochastic approximation to the Stein variational gradient by simply
    considering subsets of particles. This allows us to substantively decease
    the number of particles used in an quadratically complex computation. We
    leverage multithreading to enable distributed estimation the Stein
    variational gradient descent update.

    Particles are randomized at each iteration so that we do not observe
    particles collapsing to the same representative sample.
    """
    def __init__(
            self, n_threads, n_iters, n_particles, log_p, gd, theta=None
    ):
        """Initialize the parameters of the distributed Stein sampler class.

            n_threads (int): The number of threads to use in the distributed
                Stein variational gradient descent algorithm.
            n_iters (int): The number of iterations of Stein variational
                gradient descent to perform on a fixed subset of particles.
            n_particles (int): The number of particles to use in the algorithm.
                This is equivalently the number of samples to generate from the
                target distribution.
            log_p (TensorFlow tensor): A TensorFlow object corresponding to the
                log-posterior distribution from which parameters wish to be
                sampled. We only need to define the log-posterior up to an
                addative constant since we'll simply take the gradient with
                respect to the inputs and this term will vanish.
            gd (AbstractGradientDescent): An object that inherits from the
                abstract gradient descent object defined within the Stein
                library. This class is used to determine how to perturb the
                particles once the optimal perturbation direction. For instance,
                we might choose to update the particles according to the Adam
                optimizer scheme.
            theta (numpy array, optional): An optional parameter corresponding
                to the initial values of the particles. The dimension of this
                array (if it is provided) should be the number of particles by
                the number of random variables (parameters) to be sampled. If
                this value is not provided, then the initial particles will be
                generated by sampling from a multivariate standard normal
                distribution.
        """
        # Call the super class.
        super().__init__(n_particles, log_p, theta)
        # Number of threads and the number of Stein variational gradient descent
        # iterations to perform.
        self.n_threads = n_threads
        self.n_iters = n_iters
        # Number of particles that each worker should train with.
        self.particles_per_worker = self.n_particles // self.n_threads

        # Create the workers.
        self.workers = []
        for i in range(self.n_threads):
            self.workers.append(SteinSampler(
                self.particles_per_worker, log_p, gd
            ))

    def __train_on_batch(self, batch_feed, thread_index):
        """Internal method that will call the `train_on_batch` method the
        requisite number of times on a fixed subset of particles.
        """
        for _ in range(self.n_iters):
            self.workers[thread_index].train_on_batch(batch_feed)

    def train_on_batch(self, batch_feed):
        """Implementation of abstract base class method."""
        # Shuffle the first dimension of the particles to avoid collapse to
        # identical samples.
        shuffle_idx = np.random.permutation(self.n_particles)
        self.theta = {v: x[shuffle_idx] for v, x in self.theta.items()}
        # Create the threads and append them to a list that can be later joined.
        threads = []
        for i in range(self.n_threads):
            # Assign the worker a given subset of the particles to update.
            self.workers[i].theta = {
                v: x[i:i+self.particles_per_worker]
                for v, x in self.theta.items()
            }
            # Create and start the thread.
            threads.append(th.Thread(
                target=self.__train_on_batch,
                args=(batch_feed, i)
            ))
            threads[i].start()

        # Block the main program to ensure that all threads complete before
        # proceeding.
        for t in threads:
            t.join()

        # Now copy the updated parameters back to the global dictionary of
        # particles.
        for v in self.theta:
            for i in range(self.n_threads):
                self.theta[v][i:i+self.particles_per_worker] = (
                    self.workers[i].theta[v]
                )

